{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "SpeechGeneration.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyNMaXQx/Z5boaexOq0hhHSv",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/gianluigimazzaglia/SpeechGeneration_friends/blob/main/SpeechGeneration.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 37,
      "metadata": {
        "id": "SMZuKQIt2syY"
      },
      "outputs": [],
      "source": [
        "# Imports\n",
        "import random\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.backends.cudnn as cudnn; cudnn.benchmark = True"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!gdown --id 1syp8QemrZ4sZtaY-2-DwIXx0630VhK4l"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zwzSC1Kd2_T7",
        "outputId": "68cbf7fb-9a57-468b-c51e-15a93f97744c"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/gdown/cli.py:131: FutureWarning: Option `--id` was deprecated in version 4.3.1 and will be removed in 5.0. You don't need to pass it anymore to use a file ID.\n",
            "  category=FutureWarning,\n",
            "Downloading...\n",
            "From: https://drive.google.com/uc?id=1syp8QemrZ4sZtaY-2-DwIXx0630VhK4l\n",
            "To: /content/Friends_Transcript.txt\n",
            "100% 4.90M/4.90M [00:00<00:00, 117MB/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Options\n",
        "data_path = \"Friends_Transcript.txt\"\n",
        "batch_size = 8\n",
        "batch_seq_len = 16\n",
        "embed_size = 512\n",
        "rnn_size = 1024"
      ],
      "metadata": {
        "id": "cW1Exhx64UnN"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Load data\n",
        "with open(data_path) as f:\n",
        "    text = f.read()\n",
        "# Skip notice\n",
        "text = text[180:]  # we eliminated the first part of the text that represents just a description meaningless"
      ],
      "metadata": {
        "id": "hoTCs3It4jYv"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "text[:200]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        },
        "id": "Qnypc91v4o0O",
        "outputId": "4229fb0a-5398-48de-a73b-9e61e75f10ab"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "\"Monica: There's nothing to tell! He's just some guy I work with!\\nJoey: C'mon, you're going out with the guy! There's gotta be something wrong with him!\\nChandler: All right Joey, be nice. So does he ha\""
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "### Replace punctuation with tokens ###\n",
        "# Create token dictionary\n",
        "token_dict = {\".\": \"|fullstop|\",\n",
        "              \",\": \"|comma|\",\n",
        "              \"\\\"\": \"|quote|\",\n",
        "              \";\": \"|semicolon|\",\n",
        "              \"!\": \"|exclamation|\",\n",
        "              \"?\": \"|question|\",\n",
        "              \"(\": \"|leftparen|\",\n",
        "              \")\": \"|rightparen|\",\n",
        "              \"--\": \"|dash|\",\n",
        "              \"\\n\": \"|newline|\"\n",
        "}\n",
        "# Replace punctuation\n",
        "for punct, token in token_dict.items():\n",
        "    text = text.replace(punct, f' {token} ')"
      ],
      "metadata": {
        "id": "tRVGaAvjE5M0"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Print sample\n",
        "text[:200]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        },
        "id": "ZPY-Pzmh6XBF",
        "outputId": "c8342b3d-9fac-4cab-b31d-0ae8619b5b8d"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "\"Monica: There's nothing to tell |exclamation|  He's just some guy I work with |exclamation|  |newline| Joey: C'mon |comma|  you're going out with the guy |exclamation|  There's gotta be something wron\""
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "### Compute vocabulary ###\n",
        "\n",
        "# Split words\n",
        "words = text.split(\" \")\n",
        "# Remove empty words\n",
        "words = [word for word in words if len(word) > 0]\n",
        "# Remove duplicates\n",
        "vocab = list(set(words))  #we have just unique word in vocab"
      ],
      "metadata": {
        "id": "0aEThXpH6rfV"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for i, w in enumerate(vocab[:5]):\n",
        "  print(i, w)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ar2hO0Ap69LM",
        "outputId": "db00f55e-bde7-457f-b86d-d01b5fad2600"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0 EYES\n",
            "1 shoos\n",
            "2 glass]\n",
            "3 Fish\n",
            "4 Marge:\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Create maps between words\n",
        "vocab_to_int = {word: i for i,word in enumerate(vocab)}\n",
        "int_to_vocab = {i: word for i,word in enumerate(vocab)}\n",
        "#vocab_to_int['bright']   for example this work is a key and it is transformed in integ 1"
      ],
      "metadata": {
        "id": "PcCQ5o6f8KKA"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Compute number of words\n",
        "num_words = len(vocab)\n",
        "print(num_words)  # it is the total of unique words"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "r_M525YH8OiU",
        "outputId": "72219bcf-938f-4e9d-eade-5e102f82bcf7"
      },
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "27750\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(len([word for word in text.split(\" \") if len(word) > 0]))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Pua1I9X48Tgn",
        "outputId": "8f5aed47-537f-4f00-f8b5-415ecf7356df"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1207503\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Convert text to integers\n",
        "text_ints = [vocab_to_int[word] for word in text.split(\" \") if len(word) > 0] \n",
        "text_ints[:5]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9SAJzKsk8mVF",
        "outputId": "62947a58-a908-473d-a439-771af55e5388"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[8517, 18790, 1057, 9099, 18225]"
            ]
          },
          "metadata": {},
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "len(text_ints) #of course the length of this must be equals to the length of num of word because they are just converted in integer"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "70c1Wiou8oVB",
        "outputId": "1ec3f592-1d39-494a-973c-63ae2073bb87"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "1207503"
            ]
          },
          "metadata": {},
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Estimate average scene length\n",
        "import re\n",
        "\n",
        "scene = re.findall(r'\\[Scene.*?\\]', text)\n",
        "\n",
        "num_scenes = len(scene)\n",
        "print(len(text_ints)/num_scenes)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FdnWfJIk8e7Y",
        "outputId": "d4a5b5f9-c588-4455-978c-48a25b23c98b"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "389.8944139489829\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "new_text = [word for word in text.split(\" \") if len(word) > 0]\n",
        "inputs = new_text[:10]\n",
        "target = new_text[1:10]\n",
        "\n",
        "print(inputs)\n",
        "print(target)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "u2kbUBI78wuk",
        "outputId": "000d0559-3ab3-4503-c33e-3f95388eadf2"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['Monica:', \"There's\", 'nothing', 'to', 'tell', '|exclamation|', \"He's\", 'just', 'some', 'guy']\n",
            "[\"There's\", 'nothing', 'to', 'tell', '|exclamation|', \"He's\", 'just', 'some', 'guy']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Set scene length (should be multiple of batch_seq_len)\n",
        "scene_length = 256"
      ],
      "metadata": {
        "id": "gPjCDZcy85Nh"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Compute batches\n",
        "# Needs to be a function so we can compute different batches at different epochs\n",
        "def get_batches(text_ints, scene_length, batch_size, batch_seq_len):\n",
        "    # Compute number of \"scenes\"\n",
        "    num_scenes = len(text_ints)//scene_length\n",
        "    # Compute targets for each word (with fake target for final word)\n",
        "    text_targets = text_ints[1:] + [text_ints[0]]\n",
        "    # Split text into scenes (input and targets)\n",
        "    scene_inputs = [text_ints[i * scene_length : (i+1) * scene_length] for i in range(num_scenes)]\n",
        "    scene_targets = [text_targets[i*scene_length:(i+1)*scene_length] for i in range(num_scenes)]\n",
        "    # Split scenes into mini-sequences of length batch_seq_len\n",
        "    num_mini_sequences = scene_length//batch_seq_len\n",
        "    scene_inputs = [[scene[i*batch_seq_len:(i+1)*batch_seq_len] for i in range(num_mini_sequences)] for scene in scene_inputs]\n",
        "    scene_targets = [[scene[i*batch_seq_len:(i+1)*batch_seq_len] for i in range(num_mini_sequences)] for scene in scene_targets]\n",
        "    # Build batches\n",
        "    num_batch_groups = len(scene_inputs)//batch_size\n",
        "    batches = []\n",
        "    for i in range(num_batch_groups):\n",
        "        # Get the scenes in this group\n",
        "        group_scene_inputs = scene_inputs[i*batch_size:(i+1)*batch_size]\n",
        "        group_scene_targets = scene_targets[i*batch_size:(i+1)*batch_size]\n",
        "        # Build batches for each mini-sequence\n",
        "        for j in range(num_mini_sequences):\n",
        "            reset_state = (j == 0)\n",
        "            batch_inputs = torch.LongTensor([group_scene_inputs[k][j] for k in range(batch_size)])\n",
        "            batch_targets = torch.LongTensor([group_scene_targets[k][j] for k in range(batch_size)])\n",
        "            batches.append((reset_state, batch_inputs, batch_targets))\n",
        "    # Return\n",
        "    return batches"
      ],
      "metadata": {
        "id": "rGrIDpundZ5M"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Get batches\n",
        "batches = get_batches(text_ints, scene_length, batch_size, batch_seq_len)\n",
        "batches[0][1].shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_ROAisbNdepu",
        "outputId": "bba9bcd7-1ebb-45b3-a18a-258cefc2f757"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([8, 16])"
            ]
          },
          "metadata": {},
          "execution_count": 22
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "script = [  int_to_vocab[y.item()] for y in [x for x in batches[1][1][3]] ]\n",
        "script"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7GlUNNWHdizD",
        "outputId": "660ce367-5cd8-4601-f12e-a7756d47c74b"
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['|leftparen|',\n",
              " 'to',\n",
              " 'All',\n",
              " '|rightparen|',\n",
              " 'Okay',\n",
              " '|comma|',\n",
              " 'everybody',\n",
              " '|comma|',\n",
              " 'this',\n",
              " 'is',\n",
              " 'Rachel',\n",
              " '|comma|',\n",
              " 'another',\n",
              " 'Lincoln',\n",
              " 'High',\n",
              " 'survivor']"
            ]
          },
          "metadata": {},
          "execution_count": 23
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bH4lAyqoJIAU"
      },
      "source": [
        "# Define model\n",
        "class Model(nn.Module):\n",
        "    \n",
        "    # Constructor\n",
        "    def __init__(self, num_words, embed_size, rnn_size):\n",
        "        # Call parent constructor\n",
        "        super().__init__()\n",
        "        # Store needed attributes\n",
        "        self.rnn_size = rnn_size\n",
        "        self.state = None\n",
        "        # Define modules\n",
        "        self.embedding = nn.Embedding(num_words, embed_size)\n",
        "        self.rnn = nn.LSTM(embed_size, rnn_size, batch_first=True)\n",
        "        self.decoder = nn.Linear(rnn_size, num_words)\n",
        "        # Flags\n",
        "        self.reset_next_state = False\n",
        "        \n",
        "    def reset_state(self):\n",
        "        # Mark next state to be re-initialized\n",
        "        self.reset_next_state = True\n",
        "        \n",
        "    def forward(self, x):\n",
        "        # Check state reset\n",
        "        if self.reset_next_state:\n",
        "            # Initialize state (num_layers x batch_size x rnn_size)\n",
        "            self.state = (\n",
        "                x.new_zeros(1, x.size(0), self.rnn_size).float(), \n",
        "                x.new_zeros(1, x.size(0), self.rnn_size).float())\n",
        "            # Clear flag\n",
        "            self.reset_next_state = False\n",
        "        # Embed data\n",
        "        x = self.embedding(x)\n",
        "        # Process RNN\n",
        "        state = self.state if self.state is not None else None\n",
        "        x, state = self.rnn(x, state)\n",
        "        self.state = (state[0].data, state[1].data)\n",
        "        # Compute outputs\n",
        "        x = self.decoder(x)\n",
        "        return x"
      ],
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Create model\n",
        "model = Model(num_words, embed_size, rnn_size)"
      ],
      "metadata": {
        "id": "r3mVilAgdpR2"
      },
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Setup device\n",
        "dev = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n"
      ],
      "metadata": {
        "id": "5bcZWs0Fdszr"
      },
      "execution_count": 31,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Move model to device\n",
        "model = model.to(dev)"
      ],
      "metadata": {
        "id": "LcCsWA5zdu_N"
      },
      "execution_count": 32,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Define script generation function\n",
        "def generate_script(model, seq_len, script_start):\n",
        "    # Convert punctuaction in script start\n",
        "    for punct, token in token_dict.items():\n",
        "        script_start = script_start.replace(punct, f' {token} ')\n",
        "    # Convert script start text to ints\n",
        "    script_start = [vocab_to_int[word] for word in script_start.split(\" \") if len(word) > 0]\n",
        "    # Initialize output words/tokens\n",
        "    script = script_start[:]\n",
        "    # Convert script start to tensor (BxS = 1xS)\n",
        "    script_start = torch.LongTensor(script_start).unsqueeze_(0)\n",
        "    # Process script start and generate the rest of the script\n",
        "    model.eval()\n",
        "    model.reset_state()\n",
        "    input = script_start\n",
        "    for i in range(seq_len - script_start.size(1) + 1): # we include script_start as one of the generation steps\n",
        "        # Copy input to device\n",
        "        input = input.to(dev)\n",
        "        # Pass to model\n",
        "        output = model(input) # 1xSxV\n",
        "        # Convert to word indexes\n",
        "        words = output.max(2)[1] # 1xS\n",
        "        words = words[0] # S\n",
        "        # Add each word to script\n",
        "        for j in range(words.size(0)):\n",
        "            script.append(words[j].item())\n",
        "        # Prepare next input\n",
        "        input = torch.LongTensor([words[-1]]).unsqueeze(0) # 1xS = 1x1\n",
        "    # Convert word indexes to text\n",
        "    script = ' '.join([int_to_vocab[x] for x in script])\n",
        "    # Convert punctuation tokens to symbols\n",
        "    for punct,token in token_dict.items():\n",
        "        script = script.replace(f\"{token}\", punct)\n",
        "    # Return\n",
        "    return script"
      ],
      "metadata": {
        "id": "NPMwLUGNd082"
      },
      "execution_count": 33,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "generate_script(model, 20, \"Rachel: What?\")"
      ],
      "metadata": {
        "id": "aXRLsPmjd2qq",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        },
        "outputId": "5ca781fc-ef6a-41b1-dd04-d8d9351a3f84"
      },
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "\"Rachel: What ? himself/herself crazy deaths cheekily Aunt 'good observing Awe 1989 fan Leakey's Nails runs translation It-it's terribly drip Greens Badges Organic\""
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 34
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Create optimizer\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)"
      ],
      "metadata": {
        "id": "5djphzwyen-u"
      },
      "execution_count": 35,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Initialize training history\n",
        "loss_history = []\n",
        "# Start training\n",
        "for epoch in range(20):\n",
        "    # Initialize accumulators for computing average loss/accuracy\n",
        "    epoch_loss_sum = 0\n",
        "    epoch_loss_cnt = 0\n",
        "    # Set network mode\n",
        "    model.train()\n",
        "    # Process all batches\n",
        "    for i,batch in enumerate(batches):\n",
        "        # Parse batch\n",
        "        reset_state, input, target = batch\n",
        "        # Check reset state\n",
        "        if reset_state:\n",
        "            model.reset_state()\n",
        "        # Move to device\n",
        "        input = input.to(dev)\n",
        "        target = target.to(dev)\n",
        "        # Forward\n",
        "        output = model(input)\n",
        "        # Compute loss\n",
        "        output = output.view(-1, num_words)\n",
        "        target = target.view(-1)\n",
        "        loss = F.cross_entropy(output, target)\n",
        "        # Update loss sum\n",
        "        epoch_loss_sum += loss.item()\n",
        "        epoch_loss_cnt += 1\n",
        "        # Backward and optimize\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "    # Shift sequence and recompute batches\n",
        "    shift_point = random.randint(1, len(text_ints)-1)\n",
        "    text_ints = text_ints[:shift_point] + text_ints[shift_point:]\n",
        "    batches = get_batches(text_ints, scene_length, batch_size, batch_seq_len)\n",
        "    # Epoch end - compute average epoch loss\n",
        "    avg_loss = epoch_loss_sum/epoch_loss_cnt\n",
        "    print(f\"Epoch: {epoch+1}, loss: {epoch_loss_sum/epoch_loss_cnt:.4f}\")\n",
        "    print(\"Test sample:\")\n",
        "    print(\"---------------------------------------------------------------\")\n",
        "    print(generate_script(model, scene_length, \"Monica:\"))\n",
        "    print(\"---------------------------------------------------------------\")\n",
        "    # Add to histories\n",
        "    loss_history.append(avg_loss)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 399
        },
        "id": "bHFGSvfeesjX",
        "outputId": "a956aa72-5f0d-4403-88ba-5b11845f0424"
      },
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-36-19bea4669c3d>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     29\u001b[0m         \u001b[0;31m# Backward and optimize\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     30\u001b[0m         \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 31\u001b[0;31m         \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     32\u001b[0m         \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     33\u001b[0m     \u001b[0;31m# Shift sequence and recompute batches\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/_tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    361\u001b[0m                 \u001b[0mcreate_graph\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    362\u001b[0m                 inputs=inputs)\n\u001b[0;32m--> 363\u001b[0;31m         \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    364\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    365\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mregister_hook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    173\u001b[0m     Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass\n\u001b[1;32m    174\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad_tensors_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 175\u001b[0;31m         allow_unreachable=True, accumulate_grad=True)  # Calls into the C++ engine to run the backward pass\n\u001b[0m\u001b[1;32m    176\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    177\u001b[0m def grad(\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Generate script\n",
        "print(generate_script(model, scene_length, \"Monica: Really? \"))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 189
        },
        "id": "BQKrBlJFycxX",
        "outputId": "e28f7ebb-daf3-44b0-80de-164150fe48cf"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-1-cfaa3007c218>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# Generate script\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgenerate_script\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mscene_length\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"Monica: Really? \"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m: name 'generate_script' is not defined"
          ]
        }
      ]
    }
  ]
}